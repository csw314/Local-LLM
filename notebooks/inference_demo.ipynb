{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd4cee5",
   "metadata": {},
   "source": [
    "# Notebook: USE a Fine-Tuned BERT Model for Inference on New Data\n",
    "\n",
    "### Objective:\n",
    "\n",
    "1. Load a fine-tuned model + metadata\n",
    "2. Tokenize new data\n",
    "3. Run inference\n",
    "4. Save predictions to CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80ca6b",
   "metadata": {},
   "source": [
    "### Step 1 - Imports and Setup\n",
    "\n",
    "In this step we: \n",
    "- Import `pandas`.\n",
    "- Import utility functions from `local_llm.training.text_finetune`.\n",
    "- Define \n",
    "    - Where the fine-tuned model artifacts live \n",
    "    - Where the unlabeled data is stored.\n",
    "    - Whch columns should be concatenated to form the text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd85402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from local_llm.training.text_finetune import (\n",
    "    set_seed,\n",
    "    load_finetune_meta,\n",
    "    load_finetuned_classifier_for_inference,\n",
    "    encode_unlabeled_dataframe,\n",
    "    predict_unlabeled_tensors,\n",
    "    merge_unlabeled_with_predictions,\n",
    "    export_unlabeled_predictions_csv,\n",
    ")\n",
    "\n",
    "print(\"✅ Imports complete.\")\n",
    "\n",
    "# Directory where the fine-tuning artifacts were saved by the training notebook\n",
    "finetune_dir = Path(\"C:/Users/Cameron.Webster/Python/local-llm/artifacts/finetune_large_bert\")\n",
    "\n",
    "# Path to UNLABELED data you want to classify\n",
    "unlabeled_csv_path = Path(\"C:/Users/Cameron.Webster/Python/local-llm/data/wbs_inference_data.csv\")   # adjust if needed\n",
    "\n",
    "# Text columns to concatenate (must match what you used when training)\n",
    "text_cols = (\"wbs_name\", \"keyword\")\n",
    "\n",
    "print(f\"Fine-tune artifacts directory: {finetune_dir.resolve()}\")\n",
    "print(f\"Unlabeled data CSV: {unlabeled_csv_path.resolve()}\")\n",
    "print(f\"Text columns used for the model: {text_cols}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d88fc",
   "metadata": {},
   "source": [
    "### Step 2 - Load Metadata, Rebuild Config, and load the fine-tuned model\n",
    "\n",
    "When you fine-tuned BERT, the training scripts saved:\n",
    "- `classifier_full.pt` - full classifier (BERT encoder + classifier head) weights.\n",
    "- `finetune_meta.json` - metadata about labels and training setup.\n",
    "\n",
    "Here we: \n",
    "- Load the metadata to reconstruct label mappings (`label_to_id`, `id_to_label`).\n",
    "- Rebuild a `FineTuneConfig` suitable for inference.\n",
    "- Rebuild the model architecture and then laod the fine-tuned weights.\n",
    "\n",
    "This gives us the **same model** tha was trained eariler, ready to run predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: set seed for reproducibility of any stochastic ops\n",
    "set_seed(42)\n",
    "print(\"Random seed set to 42.\\n\")\n",
    "\n",
    "# Load meta + model in one go\n",
    "model, cfg, label_to_id, id_to_label, meta = load_finetuned_classifier_for_inference(\n",
    "    output_dir=finetune_dir,\n",
    "    text_cols=text_cols,\n",
    ")\n",
    "\n",
    "print(\"✅ Fine-tuned classifier loaded.\")\n",
    "print(\"Label mapping (label_to_id):\")\n",
    "for lab, idx in label_to_id.items():\n",
    "    print(f\"  {lab!r} -> {idx}\")\n",
    "\n",
    "print(\"\\nSome key config fields for inference:\")\n",
    "print(f\"  assets_dir:      {cfg.assets_dir}\")\n",
    "print(f\"  output_dir:      {cfg.output_dir}\")\n",
    "print(f\"  max_len:         {cfg.max_len}\")\n",
    "print(f\"  pooling:         {cfg.pooling}\")\n",
    "print(f\"  finetune_policy: {cfg.finetune_policy}\")\n",
    "print(f\"  finetune_last_n: {cfg.finetune_last_n}\")\n",
    "print(f\"  device:          {cfg.device}\")\n",
    "\n",
    "# Confirm model is on the right device\n",
    "print(\"\\nModel is on device:\", next(model.parameters()).device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6586a",
   "metadata": {},
   "source": [
    "### Step 3 - Load New Unlabeled Data\n",
    "\n",
    "Now we load the CSV file that contains new examples to classify. \n",
    "\n",
    "These rows: \n",
    "- Have text columsn (e.g., `wbs_name`, `wbs_title_hierarchy`, `keyword`).\n",
    "- Do not have label columns (we want to predict them).\n",
    "\n",
    "We print:\n",
    "- Shape of the data\n",
    "- Column Names\n",
    "- First few rows\n",
    "\n",
    "This helps verify that the text columsn awe configured actually exist in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d332a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading unlabeled data from: {unlabeled_csv_path.resolve()}\")\n",
    "unlabeled_df = pd.read_csv(unlabeled_csv_path)\n",
    "\n",
    "print(\"\\n✅ Unlabeled data loaded.\")\n",
    "print(f\"Data shape: {unlabeled_df.shape[0]} rows × {unlabeled_df.shape[1]} columns\")\n",
    "print(\"Columns:\", unlabeled_df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the unlabeled data:\")\n",
    "display(unlabeled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025595c",
   "metadata": {},
   "source": [
    "### Step 4 - Encode unlabeled Text into Model-ready Tensors\n",
    "\n",
    "The model can't work directly with raw text; it needs:\n",
    "- `input_ids` - token IDs for each position in the sequence.\n",
    "- `token_type_ids` - segment IDs (for single-sentence inputs these are usually all zeros).\n",
    "- `attention_mask` - 1 where tokens are real, 0 where they are padding.\n",
    "\n",
    "In this step we: \n",
    "- Use `encode_unlabeled_dataframe` (from the library) to: \n",
    "    - Concatenate your selected text columns into a single string.\n",
    "    - Tokenize and encode each example.\n",
    "    - Return the three tensors in a dict.\n",
    "\n",
    "We then print the shapes so you can seee how many example and how long each sequence is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_tensors = encode_unlabeled_dataframe(\n",
    "    df=unlabeled_df,\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "print(\"✅ Unlabeled data encoded.\")\n",
    "print(\"Tensor shapes:\")\n",
    "for k, v in unlabeled_tensors.items():\n",
    "    print(f\"  {k}: {tuple(v.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c99ea9",
   "metadata": {},
   "source": [
    "### Step 5 - Run Inference and Collect Predictions\n",
    "\n",
    "With this text encoded and the model loaded, we can now run inference:\n",
    "- `predict_unlabeled_tensors`: \n",
    "    - Wraps the tensors into a datset and dataloader.\n",
    "    - Runs the model in evaluation mode (no gradient updates).\n",
    "    - For each example, computes:\n",
    "        - `pred_label_id` - the predicted class index\n",
    "        - `pred_label` - the human readable label(using `id_to_label`).\n",
    "        - `pred_confidence` - the model's condifence for that label.\n",
    "\n",
    "We'll inspect the fist few prediction rows to see what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23025de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = predict_unlabeled_tensors(\n",
    "    model=model,\n",
    "    unlabeled_tensors=unlabeled_tensors,\n",
    "    cfg=cfg,\n",
    "    id_to_label=id_to_label,\n",
    ")\n",
    "\n",
    "print(\"✅ Inference complete.\")\n",
    "print(f\"Number of predictions: {len(preds_df)}\")\n",
    "\n",
    "print(\"\\nFirst 5 prediction rows:\")\n",
    "display(preds_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961caf79",
   "metadata": {},
   "source": [
    "### Step 6 - Merge Predictions Back Onto the Original Rows\n",
    "\n",
    "\n",
    "The `preds_df` we just created only contains prediction-related columns.\n",
    "\n",
    "To make the results more useful, we merge predictions with the original data, so each row has:\n",
    "- The original input columns (e.g., `wbs_name`, `wbs_title_hierarchy`, `keyword`, etc.).\n",
    "- `pred_label_id` - numeric class ID.\n",
    "- `pred_label` - human-readable label.\n",
    "- `pred_confidence` - the model's confidence.\n",
    "\n",
    "This way you can filter, sort, and analyze results easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ba985",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_unlabeled_with_predictions(\n",
    "    raw_df=unlabeled_df,\n",
    "    preds_df=preds_df,\n",
    ")\n",
    "\n",
    "print(\"✅ Merged original data with predictions.\")\n",
    "print(f\"Merged shape: {merged_df.shape[0]} rows × {merged_df.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of merged data:\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c7e01",
   "metadata": {},
   "source": [
    "### Step 7 - Save Predictions to CSV and Inspect Basic Stats\n",
    "\n",
    "Finally, we save the merged predictions to disk:\n",
    "- The file is written inside the same output directory used for fine-tuning.\n",
    "- You can open it in Excel, pandas, or any other tool. \n",
    "\n",
    "We also:\n",
    "- Reload the CSV for a quick sanity check. \n",
    "- Show the distrubution of predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c509f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = export_unlabeled_predictions_csv(\n",
    "    merged_df=merged_df,\n",
    "    cfg=cfg,\n",
    "    filename=\"unlabeled_predictions.csv\",\n",
    ")\n",
    "\n",
    "print(\"✅ Predictions saved to CSV:\")\n",
    "print(output_csv_path.resolve())\n",
    "\n",
    "# Optional: reload to sanity-check\n",
    "loaded_preds = pd.read_csv(output_csv_path)\n",
    "print(f\"\\nReloaded {len(loaded_preds)} rows from saved predictions.\")\n",
    "print(\"Columns:\", loaded_preds.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows of saved predictions:\")\n",
    "display(loaded_preds.head())\n",
    "\n",
    "print(\"\\nPredicted label distribution:\")\n",
    "display(loaded_preds[\"pred_label\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
