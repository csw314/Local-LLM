{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc5ff43",
   "metadata": {},
   "source": [
    "# Notebook: Fine-tune BERT on Your Labeled CSV Data\n",
    "\n",
    "### Objective:\n",
    "\n",
    "1. Walk through the entire fine-tuning pipeline step by step\n",
    "2. Explain what's going on in accessible language\n",
    "3. Prints out shapes, label distrubtions, and smaple rows at each stage so you can \"see\" the data evolving.\n",
    "4. Uses the `local_llm`'s core functions, so the notebook stays relatively clean and focused on _workflow_, not implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2c819",
   "metadata": {},
   "source": [
    "### Step 1 - Setup and Imports \n",
    "\n",
    "In this fist step, we import all the tools we need:\n",
    "- `pandas` to load and inspect the CSV.\n",
    "- The fine-tuning helpers form `local_llm.training.text_finetune`.\n",
    "\n",
    "We'll also define the paths to your data and BERT assets later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81108fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from local_llm.training.text_finetune import (\n",
    "    FineTuneConfig,\n",
    "    set_seed,\n",
    "    prepare_label_mapping,\n",
    "    stratified_split_indices,\n",
    "    encode_splits,\n",
    "    build_dataloaders,\n",
    "    build_bert_text_classifier_from_assets,\n",
    "    train_text_classifier,\n",
    "    evaluate_on_split,\n",
    "    save_finetuned_classifier,\n",
    "    export_predictions_csv,\n",
    ")\n",
    "\n",
    "print(\"Imports complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3daa01",
   "metadata": {},
   "source": [
    "### Step 2 - Load Your Labeled Data\n",
    "\n",
    "Now we: \n",
    "1. Load the labeled CSV file into a pandas DataFrame.\n",
    "2. Print:\n",
    "    - The first few rows\n",
    "    - Shape of the data (rows, columns)\n",
    "    - The Column Names\n",
    " \n",
    "This helps confirm that:\n",
    "- The file path is correct.\n",
    "- The column names match what we expect later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d1d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\Cameron.Webster\\Python\\local-llm\\data\\wbs_training_data.csv\n",
      "\n",
      "✅ Data loaded.\n",
      "Data shape: 19857 rows × 5 columns\n",
      "\n",
      "Columns:\n",
      "['parsid', 'wbs', 'wbs_name', 'keyword', 'level_1']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "parsid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wbs",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "wbs_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "level_1",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3dae0dd6-a370-4c23-8673-56756eb0a921",
       "rows": [
        [
         "0",
         "389",
         "3.1",
         "M&O Support",
         "m o support",
         "OPC Testing and Startup"
        ],
        [
         "1",
         "389",
         "3.2",
         "DOE Support",
         "doe support",
         "OPC Testing and Startup"
        ],
        [
         "2",
         "389",
         "2.1.0",
         "Enhanced Conceptual Design (ECD)",
         "conceptual design",
         "Design and NRE"
        ],
        [
         "3",
         "389",
         "2.1.1",
         "Preliminary Design (PD)",
         "preliminary design",
         "Design and NRE"
        ],
        [
         "4",
         "389",
         "2.1.2",
         "Final Design (FD)",
         "final design",
         "Design and NRE"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsid</th>\n",
       "      <th>wbs</th>\n",
       "      <th>wbs_name</th>\n",
       "      <th>keyword</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>389</td>\n",
       "      <td>3.1</td>\n",
       "      <td>M&amp;O Support</td>\n",
       "      <td>m o support</td>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>3.2</td>\n",
       "      <td>DOE Support</td>\n",
       "      <td>doe support</td>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389</td>\n",
       "      <td>2.1.0</td>\n",
       "      <td>Enhanced Conceptual Design (ECD)</td>\n",
       "      <td>conceptual design</td>\n",
       "      <td>Design and NRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>389</td>\n",
       "      <td>2.1.1</td>\n",
       "      <td>Preliminary Design (PD)</td>\n",
       "      <td>preliminary design</td>\n",
       "      <td>Design and NRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>389</td>\n",
       "      <td>2.1.2</td>\n",
       "      <td>Final Design (FD)</td>\n",
       "      <td>final design</td>\n",
       "      <td>Design and NRE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parsid    wbs                          wbs_name             keyword  \\\n",
       "0     389    3.1                       M&O Support         m o support   \n",
       "1     389    3.2                       DOE Support         doe support   \n",
       "2     389  2.1.0  Enhanced Conceptual Design (ECD)   conceptual design   \n",
       "3     389  2.1.1           Preliminary Design (PD)  preliminary design   \n",
       "4     389  2.1.2                 Final Design (FD)        final design   \n",
       "\n",
       "                   level_1  \n",
       "0  OPC Testing and Startup  \n",
       "1  OPC Testing and Startup  \n",
       "2           Design and NRE  \n",
       "3           Design and NRE  \n",
       "4           Design and NRE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = Path(\"C:/Users/Cameron.Webster/Python/local-llm/data/wbs_training_data.csv\")  # adjust if your file lives elsewhere\n",
    "\n",
    "print(f\"Loading data from: {data_path.resolve()}\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"\\n✅ Data loaded.\")\n",
    "print(f\"Data shape: {df.shape[0]} rows × {df.shape[1]} columns\\n\")\n",
    "print(\"Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fed667",
   "metadata": {},
   "source": [
    "### Step 3 - Define Fune-tuning Configuration\n",
    "\n",
    "Here we deine a `FineTuneConfig`, which is a single object that:\n",
    "- Tells the library which text columns touse and which columns is the label. \n",
    "- Controls how we split into train/validation/test sets.\n",
    "- Point to your BERT assets directory. \n",
    "- Sets training hyperparameters like learning rate and number of epochs.\n",
    "- Defines how much BERT we fine-tune(`finetune_policy` and `finetune_last_n`).\n",
    "\n",
    "We also call `set_seed` to make the run reproducible (same random splits each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51397d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuneConfig created:\n",
      "FineTuneConfig(text_cols=('wbs_name', 'keyword'), label_col='level_1', train_frac=0.8, val_frac=0.1, seed=42, assets_dir=WindowsPath('C:/Users/Cameron.Webster/Python/local-llm/assets/bert-large-local'), max_len=256, lowercase=True, batch_size=32, epochs=5, base_lr=2e-05, weight_decay=0.01, grad_clip_norm=1.0, finetune_policy='last_n', finetune_last_n=2, train_embeddings=False, pooling='cls', head_config=ClassifierHeadConfig(hidden_sizes=(768,), dropouts=(0.15, 0.2), use_layer_norm=True, activation='gelu'), device='cuda', output_dir=WindowsPath('C:/Users/Cameron.Webster/Python/local-llm/artifacts/finetune_large_bert'), run_name='finetune_run')\n",
      "\n",
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "cfg = FineTuneConfig(\n",
    "    # Which text columns to concatenate into a single input string\n",
    "    text_cols=(\"wbs_name\", \"keyword\"),\n",
    "\n",
    "    # Which column contains the labels\n",
    "    label_col=\"level_1\",\n",
    "\n",
    "    # Where the converted BERT assets live\n",
    "    #assets_dir=Path(\"C:/Users/Cameron.Webster/Python/local-llm/assets/bert-large-local\"),\n",
    "\n",
    "    # Where to save fine-tuning outputs (models, predictions, etc.)\n",
    "    output_dir=Path(\"C:/Users/Cameron.Webster/Python/local-llm/artifacts/finetune_large_bert\"),\n",
    "\n",
    "    # Train/val/test fractions (they should sum to <= 1, remainder is test)\n",
    "    train_frac=0.8,\n",
    "    val_frac=0.10,\n",
    "\n",
    "    # Training loop hyperparameters\n",
    "    epochs=5,\n",
    "    base_lr=2e-5, \n",
    "    batch_size=32,\n",
    "\n",
    "    # How much of BERT to fine-tune\n",
    "    finetune_policy=\"last_n\",  # options: \"none\", \"last_n\", \"full\"\n",
    "    finetune_last_n=2,         # last 2 transformer layers trainable\n",
    "    train_embeddings=False,    # embeddings stay frozen\n",
    ")\n",
    "\n",
    "print(\"FineTuneConfig created:\")\n",
    "print(cfg)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(cfg.seed)\n",
    "print(f\"\\nRandom seed set to: {cfg.seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b658a9",
   "metadata": {},
   "source": [
    "### Step 4 - Map String Labels to Integer IDs\n",
    "\n",
    "Neural networks operate on numbers, not strings. \n",
    "\n",
    "Here we:\n",
    "- Convert the label columns (e.g., `\"Construction\"`, `\"Design\"`) into integers IDs.\n",
    "- Build two dictionaries:\n",
    "    - `label_to_id`: string label --> integer\n",
    "    - `id_to_label`: integer --> string label\n",
    "- Add a new column `label_id` to the DataFrame.\n",
    "\n",
    "We also print:\n",
    "- The mappings\n",
    "- How many examples there are per class (class balanace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bc95f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing label mapping using label column: 'level_1'\n",
      "\n",
      "✅ Label mapping created.\n",
      "label_to_id:\n",
      "  'Construction' -> 0\n",
      "  'D&D' -> 1\n",
      "  'Design and NRE' -> 2\n",
      "  'OPC Testing and Startup' -> 3\n",
      "  'Process Equipment' -> 4\n",
      "  'SEPM' -> 5\n",
      "  'Site Preparation' -> 6\n",
      "  'Standard Equipment' -> 7\n",
      "\n",
      "id_to_label:\n",
      "  0 -> 'Construction'\n",
      "  1 -> 'D&D'\n",
      "  2 -> 'Design and NRE'\n",
      "  3 -> 'OPC Testing and Startup'\n",
      "  4 -> 'Process Equipment'\n",
      "  5 -> 'SEPM'\n",
      "  6 -> 'Site Preparation'\n",
      "  7 -> 'Standard Equipment'\n",
      "\n",
      "Value counts for label_id (class distribution):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "06a73220-370f-4f51-b8ae-d21e260e95c7",
       "rows": [
        [
         "0",
         "2676"
        ],
        [
         "1",
         "509"
        ],
        [
         "2",
         "4112"
        ],
        [
         "3",
         "1551"
        ],
        [
         "4",
         "5198"
        ],
        [
         "5",
         "2880"
        ],
        [
         "6",
         "659"
        ],
        [
         "7",
         "2272"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    2676\n",
       "1     509\n",
       "2    4112\n",
       "3    1551\n",
       "4    5198\n",
       "5    2880\n",
       "6     659\n",
       "7    2272\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview with label_id:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "level_1",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d59291cc-8f2c-4abc-8332-a54588209602",
       "rows": [
        [
         "0",
         "OPC Testing and Startup",
         "3"
        ],
        [
         "1",
         "OPC Testing and Startup",
         "3"
        ],
        [
         "2",
         "Design and NRE",
         "2"
        ],
        [
         "3",
         "Design and NRE",
         "2"
        ],
        [
         "4",
         "Design and NRE",
         "2"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_1</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Design and NRE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Design and NRE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Design and NRE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   level_1  label_id\n",
       "0  OPC Testing and Startup         3\n",
       "1  OPC Testing and Startup         3\n",
       "2           Design and NRE         2\n",
       "3           Design and NRE         2\n",
       "4           Design and NRE         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_col = cfg.label_col\n",
    "print(f\"Preparing label mapping using label column: '{label_col}'\")\n",
    "\n",
    "df_mapped, label_to_id, id_to_label = prepare_label_mapping(df, label_col)\n",
    "\n",
    "print(\"\\n✅ Label mapping created.\")\n",
    "print(\"label_to_id:\")\n",
    "for lab, idx in label_to_id.items():\n",
    "    print(f\"  {lab!r} -> {idx}\")\n",
    "\n",
    "print(\"\\nid_to_label:\")\n",
    "for idx, lab in id_to_label.items():\n",
    "    print(f\"  {idx} -> {lab!r}\")\n",
    "\n",
    "print(\"\\nValue counts for label_id (class distribution):\")\n",
    "display(df_mapped[\"label_id\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nPreview with label_id:\")\n",
    "display(df_mapped[[label_col, \"label_id\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ffc23c",
   "metadata": {},
   "source": [
    "### Step 5 - Stratified Train / Validation / Test Split\n",
    "\n",
    "We now split the data into three sets:\n",
    "- **Training**: used to fit the model. \n",
    "- **Validation**: used to tune hyperparameters and monitor overfitting.\n",
    "- **Test**: held out until the very end for final evaluation. \n",
    "\n",
    "The split is stratified, meaning:\n",
    "- Each set keeps apprximately the same label distribution as the full dataset. \n",
    "\n",
    "We print:\n",
    "- The sizes of each split.\n",
    "- Label distributions in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082f3b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stratified indices created.\n",
      "Train size: 15886\n",
      "Val size:   1986\n",
      "Test size:  1985\n",
      "\n",
      "Label distribution in TRAIN:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9f8af573-31fa-46f4-a9dc-90912ca8e42d",
       "rows": [
        [
         "0",
         "2141"
        ],
        [
         "1",
         "407"
        ],
        [
         "2",
         "3290"
        ],
        [
         "3",
         "1241"
        ],
        [
         "4",
         "4158"
        ],
        [
         "5",
         "2304"
        ],
        [
         "6",
         "527"
        ],
        [
         "7",
         "1818"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    2141\n",
       "1     407\n",
       "2    3290\n",
       "3    1241\n",
       "4    4158\n",
       "5    2304\n",
       "6     527\n",
       "7    1818\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in VAL:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "150bc413-48c1-457a-b9fa-8a01bf480586",
       "rows": [
        [
         "0",
         "268"
        ],
        [
         "1",
         "51"
        ],
        [
         "2",
         "411"
        ],
        [
         "3",
         "155"
        ],
        [
         "4",
         "520"
        ],
        [
         "5",
         "288"
        ],
        [
         "6",
         "66"
        ],
        [
         "7",
         "227"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    268\n",
       "1     51\n",
       "2    411\n",
       "3    155\n",
       "4    520\n",
       "5    288\n",
       "6     66\n",
       "7    227\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in TEST:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b43a907c-0b51-4e9f-81fe-2b8620f2aeb0",
       "rows": [
        [
         "0",
         "267"
        ],
        [
         "1",
         "51"
        ],
        [
         "2",
         "411"
        ],
        [
         "3",
         "155"
        ],
        [
         "4",
         "520"
        ],
        [
         "5",
         "288"
        ],
        [
         "6",
         "66"
        ],
        [
         "7",
         "227"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    267\n",
       "1     51\n",
       "2    411\n",
       "3    155\n",
       "4    520\n",
       "5    288\n",
       "6     66\n",
       "7    227\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = df_mapped[\"label_id\"].values\n",
    "\n",
    "train_idx, val_idx, test_idx = stratified_split_indices(\n",
    "    labels,\n",
    "    train_frac=cfg.train_frac,\n",
    "    val_frac=cfg.val_frac,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(\"✅ Stratified indices created.\")\n",
    "print(f\"Train size: {len(train_idx)}\")\n",
    "print(f\"Val size:   {len(val_idx)}\")\n",
    "print(f\"Test size:  {len(test_idx)}\")\n",
    "\n",
    "# Inspect label distribution across splits\n",
    "train_labels = df_mapped.iloc[train_idx][\"label_id\"]\n",
    "val_labels   = df_mapped.iloc[val_idx][\"label_id\"]\n",
    "test_labels  = df_mapped.iloc[test_idx][\"label_id\"]\n",
    "\n",
    "print(\"\\nLabel distribution in TRAIN:\")\n",
    "display(train_labels.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nLabel distribution in VAL:\")\n",
    "display(val_labels.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nLabel distribution in TEST:\")\n",
    "display(test_labels.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae51819",
   "metadata": {},
   "source": [
    "### Step 6 - Encode Text with BERT Input Encoder\n",
    "\n",
    "BERT expects tokeized inputs:\n",
    "- `input_ids`: integers representing word pieces.\n",
    "- `attention_mask`: 1 for real tokens, 0 for padding.\n",
    "- `token_type_ids`: segment IDs (here mostly 0s since we use single sentences).\n",
    "\n",
    "We use:\n",
    "- `encode_splits` to:\n",
    "    - Build a tokenier based on your local BERT vocab.\n",
    "    - Turn text into tensors for each split (train/validation/test).\n",
    "\n",
    "We print:\n",
    "- Tensor shapes for each split.\n",
    "- Sequence length (should be equal `cfg.max_len`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e7ba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text splits using BERT input encoder...\n",
      "\n",
      "Split: train\n",
      "  input_ids: shape=(15886, 256), dtype=torch.int64\n",
      "  token_type_ids: shape=(15886, 256), dtype=torch.int64\n",
      "  attention_mask: shape=(15886, 256), dtype=torch.int64\n",
      "  labels: shape=(15886,), dtype=torch.int64\n",
      "\n",
      "Split: val\n",
      "  input_ids: shape=(1986, 256), dtype=torch.int64\n",
      "  token_type_ids: shape=(1986, 256), dtype=torch.int64\n",
      "  attention_mask: shape=(1986, 256), dtype=torch.int64\n",
      "  labels: shape=(1986,), dtype=torch.int64\n",
      "\n",
      "Split: test\n",
      "  input_ids: shape=(1985, 256), dtype=torch.int64\n",
      "  token_type_ids: shape=(1985, 256), dtype=torch.int64\n",
      "  attention_mask: shape=(1985, 256), dtype=torch.int64\n",
      "  labels: shape=(1985,), dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding text splits using BERT input encoder...\")\n",
    "splits = encode_splits(df_mapped, train_idx, val_idx, test_idx, cfg)\n",
    "\n",
    "for split_name, tensors in splits.items():\n",
    "    print(f\"\\nSplit: {split_name}\")\n",
    "    for key, tensor in tensors.items():\n",
    "        print(f\"  {key}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d930bda",
   "metadata": {},
   "source": [
    "### Step 7 - Build PyTorch Datasets and DataLoaders\n",
    "\n",
    "PyTorch's `DataLoader`:\n",
    "- Handles batching (groups of exaqmples processed together).\n",
    "- Optionally shuffles data (for training).\n",
    "\n",
    "We:\n",
    "- Wrap each split (train, validation, test) into `TensorDictDataset`.\n",
    "- Build `DataLoaders`s with a batch size from `cfg.batch_size`.\n",
    "\n",
    "We print:\n",
    "- Number of batches for each split.\n",
    "- The shape of one batch from the trianing loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f1ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch DataLoaders...\n",
      "Train loader: 497 batch(es) with batch_size ~ 32\n",
      "Val loader: 32 batch(es) with batch_size ~ 64\n",
      "Test loader: 32 batch(es) with batch_size ~ 64\n",
      "\n",
      "Example TRAIN batch shapes:\n",
      "  input_ids:      (32, 256)\n",
      "  token_type_ids: (32, 256)\n",
      "  attention_mask: (32, 256)\n",
      "  labels:         (32,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Building PyTorch DataLoaders...\")\n",
    "loaders = build_dataloaders(splits, cfg)\n",
    "\n",
    "for name, loader in loaders.items():\n",
    "    num_batches = len(loader)\n",
    "    print(f\"{name.capitalize()} loader: {num_batches} batch(es) with batch_size ~ {loader.batch_size}\")\n",
    "\n",
    "# Peek at one training batch to understand the tensor shapes\n",
    "train_loader = loaders[\"train\"]\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "input_ids_batch, token_type_ids_batch, attention_mask_batch, labels_batch = first_batch\n",
    "\n",
    "print(\"\\nExample TRAIN batch shapes:\")\n",
    "print(f\"  input_ids:      {tuple(input_ids_batch.shape)}\")\n",
    "print(f\"  token_type_ids: {tuple(token_type_ids_batch.shape)}\")\n",
    "print(f\"  attention_mask: {tuple(attention_mask_batch.shape)}\")\n",
    "print(f\"  labels:         {tuple(labels_batch.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccc8e1",
   "metadata": {},
   "source": [
    "### Step 8 - Load BERT and Build the Classifier\n",
    "\n",
    "Now we:\n",
    "- Load the base BERT encoder from your local assets:\n",
    "    - Reads `config.json` and `pytorch_model.bin`.\n",
    "- Wrap it in a `BertTextClassifier`, which:\n",
    "    - Uses either `[CLS]` token or mean pooling (here: `cfg.pooling`).\n",
    "    - Adds a classifier head on top to predict your label classes. \n",
    "- Apply your fine-tuning policy (e.g., train only the last N transformer layers).\n",
    "\n",
    "We print:\n",
    "- Number of labels\n",
    "- model device (CPU or GPU).\n",
    "- Pooling strategy.\n",
    "- Fine-tuning Policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08359a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT text classifier for 8 label(s)...\n",
      "\n",
      "✅ Model built.\n",
      "Pooling strategy: cls\n",
      "Model device: cuda:0\n",
      "Fine-tune policy: last_n, last_n=2, train_embeddings=False\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_to_id)\n",
    "print(f\"Building BERT text classifier for {num_labels} label(s)...\")\n",
    "\n",
    "model = build_bert_text_classifier_from_assets(cfg, num_labels=num_labels)\n",
    "\n",
    "print(\"\\n✅ Model built.\")\n",
    "print(f\"Pooling strategy: {model.pooling}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Fine-tune policy: {cfg.finetune_policy}, last_n={cfg.finetune_last_n}, train_embeddings={cfg.train_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3097bbe",
   "metadata": {},
   "source": [
    "### Step 9 - Train the Classifier\n",
    "\n",
    "**Time to train!**\n",
    "\n",
    "`train_text_classifer` will:\n",
    "- Build an optimizer (AdamW) using only trainable parameters.\n",
    "- Loop over epochs:\n",
    "    - Train on the training loader.\n",
    "    - Evaluate on the validation loader\n",
    "- Track:\n",
    "    - Training/validation loss.\n",
    "    - Training/validation accuracy.\n",
    "- Keep the best model state (based on validation accuracy). \n",
    "\n",
    "We print:\n",
    "- The per-epoch metrics (the function already does this).\n",
    "- The final training history in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661fb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "history, best_state = train_text_classifier(model, loaders, cfg)\n",
    "\n",
    "print(\"\\n✅ Training complete.\")\n",
    "\n",
    "# Convert history to a DataFrame for easier viewing\n",
    "history_df = pd.DataFrame(history)\n",
    "print(\"\\nTraining history:\")\n",
    "display(history_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9031d4",
   "metadata": {},
   "source": [
    "### Step 10 - Save the Fine-tuned Model and Metadata \n",
    "\n",
    "After training, we want to:\n",
    "- Save:\n",
    "    - The full classifier (encoder + head).\n",
    "    - The fine-tuned encoder weights in BERT format.\n",
    "    - A small JSON metadata file with label mappings and training settings. \n",
    "\n",
    "The make it easy to:\n",
    "- Reload the model later\n",
    "- Run inference in an other script or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving fine-tuned classifier and encoder weights...\")\n",
    "save_finetuned_classifier(model, best_state, cfg, label_to_id, id_to_label)\n",
    "\n",
    "print(\"\\n✅ Fine-tuned model and metadata saved to:\")\n",
    "print(cfg.output_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d272f",
   "metadata": {},
   "source": [
    "### Step 11 - Evaluate on Test Set and Inspect Predictions\n",
    "\n",
    "Now we evaluate on the test split:\n",
    "- Compute:\n",
    "    - Test loss\n",
    "    - Test accuracy\n",
    "- Collect Predictions\n",
    "    - True label ids\n",
    "    - predicted label ids\n",
    "    - Confidence scores (max softmax probability)\n",
    "\n",
    "Then: \n",
    "- Merge predictions back with the originial test rows.\n",
    "- Save the combined table as a CSV (so you can inspect errors, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model on TEST split...\")\n",
    "test_metrics, test_preds = evaluate_on_split(model, splits[\"test\"], cfg)\n",
    "\n",
    "print(\"\\n✅ Test evaluation complete.\")\n",
    "print(\"Test metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nFirst 10 prediction rows (label_id, pred_label_id, pred_confidence):\")\n",
    "display(test_preds.head(10))\n",
    "\n",
    "# Align test raw rows with test_idx\n",
    "test_raw_df = df_mapped.iloc[test_idx]\n",
    "\n",
    "print(\"\\nExporting test predictions merged with raw data to CSV...\")\n",
    "csv_path = export_predictions_csv(test_preds, test_raw_df, id_to_label, cfg, split_name=\"test\")\n",
    "\n",
    "print(\"\\n✅ Test predictions saved to CSV:\")\n",
    "print(csv_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19202c8d",
   "metadata": {},
   "source": [
    "### Step 12 - Optional Quick Sanity Check on the Output CSV\n",
    "\n",
    "Finally, we can quickly reload the exported CSV and:\n",
    "- Check a few rows\n",
    "- look at the distribution of predicted labels\n",
    "\n",
    "This helps sanity-check that:\n",
    "- The file wrote correctly.\n",
    "- THe predictions look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fd22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reloading exported predictions for sanity check...\")\n",
    "preds_loaded = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"\\nLoaded {len(preds_loaded)} rows from predictions CSV.\")\n",
    "print(\"Columns in predictions CSV:\")\n",
    "print(preds_loaded.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows from predictions CSV:\")\n",
    "display(preds_loaded.head())\n",
    "\n",
    "print(\"\\nPredicted label counts:\")\n",
    "if \"pred_label\" in preds_loaded.columns:\n",
    "    display(preds_loaded[\"pred_label\"].value_counts())\n",
    "elif \"pred_label_id\" in preds_loaded.columns:\n",
    "    display(preds_loaded[\"pred_label_id\"].value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
