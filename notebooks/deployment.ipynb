{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd4cee5",
   "metadata": {},
   "source": [
    "# Notebook: Run Inference using a Fine-Tuned BERT Model Saved for Deployment \n",
    "\n",
    "### Objective:\n",
    "\n",
    "1. Load a fine-tuned model + metadata\n",
    "2. Tokenize new data\n",
    "3. Run inference\n",
    "4. Export predictions to PostgreSQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80ca6b",
   "metadata": {},
   "source": [
    "### Step 1 - Imports and Setup\n",
    "\n",
    "In this step we: \n",
    "- Import `pandas`.\n",
    "- Import utility functions from `local_llm.training.text_finetune`.\n",
    "- Define \n",
    "    - Where the fine-tuned model artifacts live \n",
    "    - Where the unlabeled data is stored.\n",
    "    - Whch columns should be concatenated to form the text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd85402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports complete.\n",
      "Fine-tune artifacts directory: \\\\ezgugvcsimnfssta01003.file.core.usgovcloudapi.net\\nnsa-share\\odrive\\NA-1.3\\VEGA\\Data_for_SQL\\Cost Category Mapping\\finetune_large_bert\n",
      "Database: postgres,  Connection Successful: True, \n",
      "Text columns used for the model: ('wbs_name', 'identified_keywords')\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from local_llm.training.text_finetune import (\n",
    "    set_seed,\n",
    "    load_finetune_meta,\n",
    "    load_finetuned_classifier_for_inference,\n",
    "    encode_unlabeled_dataframe,\n",
    "    predict_unlabeled_tensors,\n",
    "    merge_unlabeled_with_predictions,\n",
    "    export_unlabeled_predictions_csv,\n",
    "    is_connection_alive,\n",
    "    split_for_review,\n",
    ")\n",
    "\n",
    "print(\"✅ Imports complete.\")\n",
    "\n",
    "# Directory where the fine-tuning artifacts were saved by the training notebook\n",
    "finetune_dir = Path(\"O:/NA-1.3/VEGA/Data_for_SQL/Cost Category Mapping/finetune_large_bert\")\n",
    "# Target Tabel names for export predictions\n",
    "human_review_table = \"pars_spae_local_llm_review_needed\"\n",
    "machine_approved_table = \"pars_spae_local_llm_reviewed\"\n",
    "\n",
    "# ---- Define connection parameters ----\n",
    "DB_USER = \"\"\n",
    "DB_PASS = \"\"\n",
    "DB_HOST = \"172.22.20.68\"        # e.g. \"localhost\" or \"10.0.0.5\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"postgres\"\n",
    "\n",
    "# ---- Create SQLAlchemy engine ----\n",
    "# Format: postgresql+psycopg2://user:password@host:port/dbname\n",
    "connection_string = (\n",
    "    f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Text columns to concatenate (must match what you used when training)\n",
    "text_cols = (\"wbs_name\", \"identified_keywords\")\n",
    "\n",
    "print(f\"Fine-tune artifacts directory: {finetune_dir.resolve()}\")\n",
    "print(f\"Database: {engine.url.database},  Connection Successful: {is_connection_alive(engine)}, \")\n",
    "print(f\"Text columns used for the model: {text_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d88fc",
   "metadata": {},
   "source": [
    "### Step 2 - Load Metadata, Rebuild Config, and load the fine-tuned model\n",
    "\n",
    "When you fine-tuned BERT, the training scripts saved:\n",
    "- `classifier_full.pt` - full classifier (BERT encoder + classifier head) weights.\n",
    "- `finetune_meta.json` - metadata about labels and training setup.\n",
    "\n",
    "Here we: \n",
    "- Load the metadata to reconstruct label mappings (`label_to_id`, `id_to_label`).\n",
    "- Rebuild a `FineTuneConfig` suitable for inference.\n",
    "- Rebuild the model architecture and then laod the fine-tuned weights.\n",
    "\n",
    "This gives us the **same model** tha was trained eariler, ready to run predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: set seed for reproducibility of any stochastic ops\n",
    "set_seed(42)\n",
    "print(\"Random seed set to 42.\\n\")\n",
    "\n",
    "# Load meta + model in one go\n",
    "model, cfg, label_to_id, id_to_label, meta = load_finetuned_classifier_for_inference(\n",
    "    output_dir=finetune_dir,\n",
    "    text_cols=text_cols,\n",
    ")\n",
    "\n",
    "print(\"✅ Fine-tuned classifier loaded.\")\n",
    "print(\"Label mapping (label_to_id):\")\n",
    "for lab, idx in label_to_id.items():\n",
    "    print(f\"  {lab!r} -> {idx}\")\n",
    "\n",
    "print(\"\\nSome key config fields for inference:\")\n",
    "print(f\"  assets_dir:      {cfg.assets_dir}\")\n",
    "print(f\"  output_dir:      {cfg.output_dir}\")\n",
    "print(f\"  max_len:         {cfg.max_len}\")\n",
    "print(f\"  pooling:         {cfg.pooling}\")\n",
    "print(f\"  finetune_policy: {cfg.finetune_policy}\")\n",
    "print(f\"  finetune_last_n: {cfg.finetune_last_n}\")\n",
    "print(f\"  device:          {cfg.device}\")\n",
    "\n",
    "# Confirm model is on the right device\n",
    "print(\"\\nModel is on device:\", next(model.parameters()).device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6586a",
   "metadata": {},
   "source": [
    "### Step 3 - Load New Unlabeled Data\n",
    "\n",
    "Now we load the CSV file that contains new examples to classify. \n",
    "\n",
    "These rows: \n",
    "- Have text columsn (e.g., `wbs_name`, `wbs_title_hierarchy`, `keyword`).\n",
    "- Do not have label columns (we want to predict them).\n",
    "\n",
    "We print:\n",
    "- Shape of the data\n",
    "- Column Names\n",
    "- First few rows\n",
    "\n",
    "This helps verify that the text columsn awe configured actually exist in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d332a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading unlabeled data from: {engine.url.database}\")\n",
    "# ---- Define your SQL query ----\n",
    "sql = text(\"\"\"\n",
    "SELECT \n",
    "    a.parsid, \n",
    "    a.wbs,\n",
    "    a.wbs_name, \n",
    "    a.identified_keywords\n",
    "FROM silver.pars_spae_l1_mapping AS a\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM silver.pars_spae_local_llm_reviewed AS b\n",
    "    WHERE b.parsid = a.parsid\n",
    "      AND b.wbs = a.wbs\n",
    "      AND b.wbs_name = a.wbs_name\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    unlabeled_df = pd.read_sql(sql, conn)\n",
    "\n",
    "# remove quotes from keywords in col: identified_keywords\n",
    "unlabeled_df['identified_keywords'] = unlabeled_df['identified_keywords'].str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# clean up whitespace in col: identified_keywords\n",
    "unlabeled_df['identified_keywords'] = unlabeled_df['identified_keywords'].str.split().str.join(' ')\n",
    "\n",
    "print(\"\\n✅ Unlabeled data loaded.\")\n",
    "print(f\"Data shape: {unlabeled_df.shape[0]} rows × {unlabeled_df.shape[1]} columns\")\n",
    "print(\"Columns:\", unlabeled_df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the unlabeled data:\")\n",
    "display(unlabeled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025595c",
   "metadata": {},
   "source": [
    "### Step 4 - Encode unlabeled Text into Model-ready Tensors\n",
    "\n",
    "The model can't work directly with raw text; it needs:\n",
    "- `input_ids` - token IDs for each position in the sequence.\n",
    "- `token_type_ids` - segment IDs (for single-sentence inputs these are usually all zeros).\n",
    "- `attention_mask` - 1 where tokens are real, 0 where they are padding.\n",
    "\n",
    "In this step we: \n",
    "- Use `encode_unlabeled_dataframe` (from the library) to: \n",
    "    - Concatenate your selected text columns into a single string.\n",
    "    - Tokenize and encode each example.\n",
    "    - Return the three tensors in a dict.\n",
    "\n",
    "We then print the shapes so you can seee how many example and how long each sequence is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_tensors = encode_unlabeled_dataframe(\n",
    "    df=unlabeled_df,\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "print(\"✅ Unlabeled data encoded.\")\n",
    "print(\"Tensor shapes:\")\n",
    "for k, v in unlabeled_tensors.items():\n",
    "    print(f\"  {k}: {tuple(v.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c99ea9",
   "metadata": {},
   "source": [
    "### Step 5 - Run Inference and Collect Predictions\n",
    "\n",
    "With this text encoded and the model loaded, we can now run inference:\n",
    "- `predict_unlabeled_tensors`: \n",
    "    - Wraps the tensors into a datset and dataloader.\n",
    "    - Runs the model in evaluation mode (no gradient updates).\n",
    "    - For each example, computes:\n",
    "        - `pred_label_id` - the predicted class index\n",
    "        - `pred_label` - the human readable label(using `id_to_label`).\n",
    "        - `pred_confidence` - the model's condifence for that label.\n",
    "\n",
    "We'll inspect the fist few prediction rows to see what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23025de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = predict_unlabeled_tensors(\n",
    "    model=model,\n",
    "    unlabeled_tensors=unlabeled_tensors,\n",
    "    cfg=cfg,\n",
    "    id_to_label=id_to_label,\n",
    ")\n",
    "\n",
    "print(\"✅ Inference complete.\")\n",
    "print(f\"Number of predictions: {len(preds_df)}\")\n",
    "\n",
    "print(\"\\nFirst 5 prediction rows:\")\n",
    "display(preds_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961caf79",
   "metadata": {},
   "source": [
    "### Step 6 - Merge Predictions Back Onto the Original Rows\n",
    "\n",
    "\n",
    "The `preds_df` we just created only contains prediction-related columns.\n",
    "\n",
    "To make the results more useful, we merge predictions with the original data, so each row has:\n",
    "- The original input columns (e.g., `wbs_name`, `wbs_title_hierarchy`, `keyword`, etc.).\n",
    "- `pred_label_id` - numeric class ID.\n",
    "- `pred_label` - human-readable label.\n",
    "- `pred_confidence` - the model's confidence.\n",
    "\n",
    "This way you can filter, sort, and analyze results easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ba985",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_unlabeled_with_predictions(\n",
    "    raw_df=unlabeled_df,\n",
    "    preds_df=preds_df,\n",
    ")\n",
    "\n",
    "print(\"✅ Merged original data with predictions.\")\n",
    "print(f\"Merged shape: {merged_df.shape[0]} rows × {merged_df.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of merged data:\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c7e01",
   "metadata": {},
   "source": [
    "### Step 7 - Inspect Prediction Stats, Split for Human Review and Export to SQL\n",
    "\n",
    "Finally, we save the merged predictions to disk:\n",
    "Finally, we split the predictions into two tables and load to SQL\n",
    "- Table one: predictions written directly into SQL as \"machine_approved.\"\n",
    "- Table two: predicitons written to a staging table to await \"human-in-the-loop\" review. \n",
    "\n",
    "We also:\n",
    "- Append\n",
    "- Reload the predictions for a quick sanity check. \n",
    "- Show the distrubution of predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c509f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = export_unlabeled_predictions_csv(\n",
    "    merged_df=merged_df,\n",
    "    cfg=cfg,\n",
    "    filename=\"unlabeled_predictions.csv\",\n",
    ")\n",
    "\n",
    "machine_verified_df, for_human_review_df = split_for_review(\n",
    "    merged_df=merged_df,\n",
    "    conf_col=\"pred_confidence\", \n",
    "    threshold=0.9)\n",
    "\n",
    "# Push Machine Verified DataFrame to PostgreSQL\n",
    "machine_verified_df.to_sql(\n",
    "    name=machine_approved_table,\n",
    "    con=engine,\n",
    "    schema=\"silver\",            # or your schema name\n",
    "    if_exists=\"fail\",           # options: 'fail', 'replace', 'append'\n",
    "    index=False                 # don't write DataFrame index as a column\n",
    ")\n",
    "print(\"✅ Machine Verified Predictions exported to PostgreSQL:\")\n",
    "\n",
    "# Push Machine Verified DataFrame to PostgreSQL\n",
    "for_human_review_df.to_sql(\n",
    "    name=human_review_table,\n",
    "    con=engine,\n",
    "    schema=\"change\",            # or your schema name\n",
    "    if_exists=\"fail\",           # options: 'fail', 'replace', 'append'\n",
    "    index=False                 # don't write DataFrame index as a column\n",
    ")\n",
    "print(\"✅ Human Review Predictions exported to PostgreSQL:\")\n",
    "\n",
    "# Optional: reload to sanity-check\n",
    "with engine.connect() as conn:\n",
    "    loaded_preds = pd.read_sql(\"SELECT * From silver.\" + machine_approved_table, conn)\n",
    "loaded_preds = pd.read_csv(output_csv_path)\n",
    "print(f\"\\nReloaded {len(loaded_preds)} rows from Machine Verified predictions.\")\n",
    "print(\"Columns:\", loaded_preds.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows of Machine Verified predictions:\")\n",
    "display(loaded_preds.head())\n",
    "\n",
    "print(\"\\nPredicted Machine Verified distribution:\")\n",
    "display(loaded_preds[\"pred_label\"].value_counts())\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    loaded_preds = pd.read_sql(\"SELECT * From silver.\" + machine_approved_table, conn)\n",
    "loaded_preds = pd.read_csv(output_csv_path)\n",
    "print(f\"\\nReloaded {len(loaded_preds)} rows from Human Review predictions.\")\n",
    "print(\"Columns:\", loaded_preds.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows of Human Review predictions:\")\n",
    "display(loaded_preds.head())\n",
    "\n",
    "print(\"\\nPredicted Human Review distribution:\")\n",
    "display(loaded_preds[\"pred_label\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
