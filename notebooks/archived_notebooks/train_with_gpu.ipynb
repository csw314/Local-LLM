{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import local_llm as lllm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assets_dir = r\"C:/Users/Cameron.Webster/Python/local-llm/assets/bert-base-local1\"\n",
    "\n",
    "encoder = lllm.build_bert_input_encoder(assets_dir, max_len=256, lowercase=True)\n",
    "\n",
    "texts = [\"This is a test.\", \"Another example.\"]\n",
    "encoded = [encoder.encode(t) for t in texts]\n",
    "\n",
    "input_ids = torch.tensor([e.input_ids for e in encoded], device=device)\n",
    "attention_mask = torch.tensor([e.attention_mask for e in encoded], device=device)\n",
    "token_type_ids = torch.tensor([e.token_type_ids for e in encoded], device=device)\n",
    "\n",
    "num_labels = 8\n",
    "model = lllm.BertTextClassifier.from_pretrained(\n",
    "    assets_dir,\n",
    "    num_labels=num_labels,\n",
    "    pooling=\"cls\",\n",
    ")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "labels = torch.randint(0, num_labels, (input_ids.size(0),), device=device)\n",
    "\n",
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    "    labels=labels,\n",
    ")\n",
    "loss = out[\"loss\"]\n",
    "loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
